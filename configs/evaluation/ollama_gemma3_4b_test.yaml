experiment_name: ollama_gemma3_4b_test
description: Ollama test with gemma3:4b model (3.3GB) for performance evaluation
mode: accuracy

# Input configuration - using existing benchmark
benchmark_paths:
  - data/benchmarks/collected/01_logic_reasoning/easy/collected_samples.json

output_dir: data/reports/ollama_gemma3_4b_test

# Model configuration using gemma3:4b
models:
  - name: "gemma3:4b"
    provider: ollama
    temperature: 0.7
    max_tokens: 200
    timeout: 60

# Simple transformations to test the pipeline
transformations:
  enabled_types:
    - language_translation
  
  # Simple language settings for quick test
  languages:
    - constructed_agglutinative_1
  language_complexity: 1
  
  # General settings
  seed: 1
  batch_size: 2

# Evaluation settings (reasonable for 4b model testing)
max_samples: 12                   # Reasonable sample size for 4b model
sample_seed: 1
max_concurrent_requests: 1        # Sequential processing for local models
save_interval: 5

# Analysis settings (skip for quick testing)
generate_plots: false
calculate_significance: false